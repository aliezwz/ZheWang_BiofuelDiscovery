accuracy,roc_auc,f1_score,precision,recall,confusion_matrix,classification_report,brier_score,ece,mce
0.6095890410958904,0.5742481203007519,0.5899280575539567,0.5942028985507246,0.5857142857142857,"[[48 28]
 [29 41]]","{'Biofuel': {'precision': 0.6233766233766234, 'recall': 0.631578947368421, 'f1-score': 0.6274509803921569, 'support': 76.0}, 'Not biofuel': {'precision': 0.5942028985507246, 'recall': 0.5857142857142857, 'f1-score': 0.5899280575539567, 'support': 70.0}, 'accuracy': 0.6095890410958904, 'macro avg': {'precision': 0.608789760963674, 'recall': 0.6086466165413533, 'f1-score': 0.6086895189730568, 'support': 146.0}, 'weighted avg': {'precision': 0.6093892210628362, 'recall': 0.6095890410958904, 'f1-score': 0.6094605379354856, 'support': 146.0}}",0.3852739726027397,0.29250355041590587,0.4623655913978495
